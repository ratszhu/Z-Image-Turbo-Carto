# -*- coding: utf-8 -*-
"""
æ¨ç†å¼•æ“ (APIé€‚é…ç‰ˆ)
è´Ÿè´£æ¨¡å‹çš„åŠ è½½ã€æ˜¾å­˜ä¼˜åŒ–åŠå›¾ç‰‡ç”Ÿæˆã€‚
"""
import torch
from diffusers import DiffusionPipeline # type: ignore
import gc
import time
from core.utils import detect_device, get_torch_dtype
from core.lora_manager import LoRAMerger
import config

class ZImageEngine:
    # ... (å‰é¢çš„ __init__, is_loaded, load_model ä¿æŒä¸å˜) ...
    def __init__(self):
        self.pipe = None
        self.device = None
        self.dtype = None
        self.lora_merger = None
        self.current_lora_applied = False

    def is_loaded(self):
        return self.pipe is not None

    def load_model(self):
        # ... (ä¿æŒä¸å˜) ...
        self.device = detect_device()
        self.dtype = get_torch_dtype(self.device)
        
        print(f"ğŸš€ [Engine] æ­£åœ¨åŠ è½½æ¨¡å‹... è®¾å¤‡: {self.device.upper()}, ç²¾åº¦: {self.dtype}")
        
        # æ¸…ç†æ—§æ˜¾å­˜
        if self.pipe:
            del self.pipe
            self.pipe = None
            gc.collect()
            if torch.cuda.is_available(): torch.cuda.empty_cache()
            if torch.backends.mps.is_available(): torch.mps.empty_cache()

        try:
            self.pipe = DiffusionPipeline.from_pretrained(
                config.MODEL_PATH,
                torch_dtype=self.dtype,
                trust_remote_code=True,
            )
            self.pipe.to(self.device)
            
            self.lora_merger = LoRAMerger(self.pipe)
            self.current_lora_applied = False
            
            self._apply_optimizations()
            
            print("âœ… [Engine] æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚")
            return True, f"å°±ç»ª ({self.device.upper()})"
            
        except Exception as e:
            print(f"âŒ [Engine] åŠ è½½å¤±è´¥: {e}")
            return False, str(e)

    def _apply_optimizations(self):
        """åº”ç”¨ä¼˜åŒ–ç­–ç•¥"""
        # [å…³é”®ä¿®å¤] VAE å¼ºåˆ¶ FP32
        # æ— è®ºæ˜¯åœ¨ MPS è¿˜æ˜¯ CUDA ä¸Šï¼ŒVAE åœ¨ FP16/BF16 ä¸‹éƒ½ææ˜“æº¢å‡ºå¯¼è‡´é»‘å›¾æˆ–æ¨¡ç³Š
        # å› æ­¤ï¼Œå¿…é¡»å¼ºåˆ¶ VAE è¿è¡Œåœ¨ float32 æ¨¡å¼
        if hasattr(self.pipe, "vae"):
            self.pipe.vae.to(dtype=torch.float32) # type: ignore
            # æŸäº›ç‰ˆæœ¬çš„ diffusers éœ€è¦æ˜¾å¼è®¾ç½® force_upcast
            if hasattr(self.pipe.vae.config, "force_upcast"): # type: ignore
                self.pipe.vae.config.force_upcast = True # type: ignore
            print("ğŸ‘ï¸ [Optim] VAE å·²åˆ‡æ¢è‡³ FP32 (é˜²æ­¢é»‘å›¾/æ¨¡ç³Š)")

        # ç¡¬ä»¶ç‰¹å®šä¼˜åŒ–
        if self.device == "mps":
            # MPS æ˜¾å­˜è¶³å¤Ÿæ—¶å…³é—­ Tiling ä»¥è·å¾—æœ€ä½³ç”»è´¨
            pass 
        elif self.device == "cuda":
            # 12GB æ˜¾å­˜ç”¨æˆ·å¿…é¡»å¼€å¯ CPU Offload
            # è¿™ä¼šå°†æš‚æ—¶ä¸ç”¨çš„æ¨¡å‹å±‚ç§»åˆ°å†…å­˜ï¼Œè…¾å‡ºæ˜¾å­˜ç»™ VAE è§£ç 
            self.pipe.enable_model_cpu_offload() # type: ignore
            
            # [å»ºè®®å¼€å¯] VAE Tiling ä¹Ÿæ˜¯é˜²æ­¢ 12G æ˜¾å­˜è§£ç çˆ†æ˜¾å­˜/é»‘å±çš„å…³é”®
            if hasattr(self.pipe, "enable_vae_tiling"):
                self.pipe.enable_vae_tiling() # type: ignore
                print("ğŸ§  [Optim] CUDA: VAE Tiling å·²å¼€å¯ (èŠ‚çœæ˜¾å­˜)")
            
            print("ğŸ§  [Optim] CUDA: CPU Offload å·²å¼€å¯")

    # ... (åé¢çš„ update_lora å’Œ generate ä¿æŒä¸å˜) ...
    def update_lora(self, enable, scale):
        if not self.is_loaded(): return
        if (not enable and self.current_lora_applied) or (enable and self.current_lora_applied):
            print("ğŸ”„ [Engine] LoRA å˜æ›´ï¼Œé‡è½½æ¨¡å‹...")
            self.load_model()
            if enable:
                self.lora_merger.load_lora_weights(config.LORA_PATH, scale) # type: ignore
                self.current_lora_applied = True
        elif enable and not self.current_lora_applied:
            self.lora_merger.load_lora_weights(config.LORA_PATH, scale) # type: ignore
            self.current_lora_applied = True

    def generate(self, prompt, neg_prompt, steps, cfg, width, height, seed, seed_mode):
        start_time = time.time()
        gc.collect()
        if self.device == "mps": torch.mps.empty_cache()
        if self.device == "cuda": torch.cuda.empty_cache()

        if seed_mode == "random" or seed == -1:
            actual_seed = torch.randint(0, 2**32 - 1, (1,)).item()
        else:
            actual_seed = int(seed)
            
        gen_device = "cpu" if self.device == "mps" else self.device
        generator = torch.Generator(gen_device).manual_seed(actual_seed) # type: ignore

        print(f"ğŸ¨ [Generate] å°ºå¯¸: {width}x{height} | æ­¥æ•°: {steps} | ç§å­: {actual_seed}")

        try:
            image = self.pipe(
                prompt=prompt,
                negative_prompt=neg_prompt,
                num_inference_steps=steps,
                guidance_scale=cfg,
                width=width,
                height=height,
                generator=generator
            ).images[0] # type: ignore
            
            duration = time.time() - start_time
            
            return {
                "success": True,
                "image": image,
                "seed": actual_seed,
                "duration": round(duration, 2)
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }